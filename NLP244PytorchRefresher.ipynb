{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOPhfZC1oAcvUABb4faMc28",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kingb12/nlp244-section1-test/blob/main/NLP244PytorchRefresher.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qb8EWdpWaLdu",
        "outputId": "55cc577e-6680-45f1-da2e-4c24d624b383"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "\u001b[0m\u001b[01;34mnlp244-section1-test\u001b[0m/\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "%ls /content/drive/MyDrive/colab"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "assert torch.cuda.is_available(), \"not on a GPU runtime!\"\n",
        "print(torch.__version__)\n",
        "torch.manual_seed(1234)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WHjKODFoaZll",
        "outputId": "9886741e-a935-478a-8068-ac8ba7d5b14f"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1.13.1+cu116\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7f1176cd4e10>"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# some imports I use in my solution, not all are needed in yours\n",
        "import json\n",
        "from torch import nn, Tensor\n",
        "from typing import List, Dict, Union, Set\n",
        "from typing_extensions import TypedDict\n"
      ],
      "metadata": {
        "id": "YR017DM35i6D"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Cloning *from* GitHub"
      ],
      "metadata": {
        "id": "hv8nPWtha2MD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! git clone https://github.com/kingb12/nlp244-section1-test.git\n",
        "%cd nlp244-section1-test/\n",
        "! git fetch && git pull"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kJEl8sOna4PB",
        "outputId": "da127aaf-8376-47c8-cd83-fa401b61a6e0"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'nlp244-section1-test'...\n",
            "remote: Enumerating objects: 22, done.\u001b[K\n",
            "remote: Counting objects:   4% (1/22)\u001b[K\rremote: Counting objects:   9% (2/22)\u001b[K\rremote: Counting objects:  13% (3/22)\u001b[K\rremote: Counting objects:  18% (4/22)\u001b[K\rremote: Counting objects:  22% (5/22)\u001b[K\rremote: Counting objects:  27% (6/22)\u001b[K\rremote: Counting objects:  31% (7/22)\u001b[K\rremote: Counting objects:  36% (8/22)\u001b[K\rremote: Counting objects:  40% (9/22)\u001b[K\rremote: Counting objects:  45% (10/22)\u001b[K\rremote: Counting objects:  50% (11/22)\u001b[K\rremote: Counting objects:  54% (12/22)\u001b[K\rremote: Counting objects:  59% (13/22)\u001b[K\rremote: Counting objects:  63% (14/22)\u001b[K\rremote: Counting objects:  68% (15/22)\u001b[K\rremote: Counting objects:  72% (16/22)\u001b[K\rremote: Counting objects:  77% (17/22)\u001b[K\rremote: Counting objects:  81% (18/22)\u001b[K\rremote: Counting objects:  86% (19/22)\u001b[K\rremote: Counting objects:  90% (20/22)\u001b[K\rremote: Counting objects:  95% (21/22)\u001b[K\rremote: Counting objects: 100% (22/22)\u001b[K\rremote: Counting objects: 100% (22/22), done.\u001b[K\n",
            "remote: Compressing objects:   6% (1/15)\u001b[K\rremote: Compressing objects:  13% (2/15)\u001b[K\rremote: Compressing objects:  20% (3/15)\u001b[K\rremote: Compressing objects:  26% (4/15)\u001b[K\rremote: Compressing objects:  33% (5/15)\u001b[K\rremote: Compressing objects:  40% (6/15)\u001b[K\rremote: Compressing objects:  46% (7/15)\u001b[K\rremote: Compressing objects:  53% (8/15)\u001b[K\rremote: Compressing objects:  60% (9/15)\u001b[K\rremote: Compressing objects:  66% (10/15)\u001b[K\rremote: Compressing objects:  73% (11/15)\u001b[K\rremote: Compressing objects:  80% (12/15)\u001b[K\rremote: Compressing objects:  86% (13/15)\u001b[K\rremote: Compressing objects:  93% (14/15)\u001b[K\rremote: Compressing objects: 100% (15/15)\u001b[K\rremote: Compressing objects: 100% (15/15), done.\u001b[K\n",
            "remote: Total 22 (delta 5), reused 16 (delta 4), pack-reused 0\u001b[K\n",
            "Unpacking objects:   4% (1/22)   \rUnpacking objects:   9% (2/22)   \rUnpacking objects:  13% (3/22)   \rUnpacking objects:  18% (4/22)   \rUnpacking objects:  22% (5/22)   \rUnpacking objects:  27% (6/22)   \rUnpacking objects:  31% (7/22)   \rUnpacking objects:  36% (8/22)   \rUnpacking objects:  40% (9/22)   \rUnpacking objects:  45% (10/22)   \rUnpacking objects:  50% (11/22)   \rUnpacking objects:  54% (12/22)   \rUnpacking objects:  59% (13/22)   \rUnpacking objects:  63% (14/22)   \rUnpacking objects:  68% (15/22)   \rUnpacking objects:  72% (16/22)   \rUnpacking objects:  77% (17/22)   \rUnpacking objects:  81% (18/22)   \rUnpacking objects:  86% (19/22)   \rUnpacking objects:  90% (20/22)   \rUnpacking objects:  95% (21/22)   \rUnpacking objects: 100% (22/22)   \rUnpacking objects: 100% (22/22), done.\n",
            "/content/nlp244-section1-test/nlp244-section1-test\n",
            "Already up to date.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%pwd"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "6EpVDC-Ra96o",
        "outputId": "a12f86d8-f61f-4ce7-a8c9-905c2f533edf"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/nlp244-section1-test/nlp244-section1-test'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Saving *to* GitHub\n",
        "\n",
        "File > Save a copy in GitHub! Note this is a distinct version from the one in your drive, and cannot be editted. You need to save your drive version to GitHub anytime you want to preserve changes that you share via GitHub!"
      ],
      "metadata": {
        "id": "BKisOq4hbXxc"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Z6k14g9ybaBE"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Some useful boiler-plate for exercises"
      ],
      "metadata": {
        "id": "WACUpQIte8nn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch import nn, Tensor\n",
        "\n",
        "def describe(x: Tensor) -> None:\n",
        "    print(\"Type: {}\".format(x.type()))\n",
        "    print(\"Shape/size: {}\".format(x.shape))\n",
        "    print(\"Values: \\n{}\".format(x))\n",
        "\n",
        "describe(torch.randn(size=(3, 4)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ajfjUNy2fHUu",
        "outputId": "666cba18-c766-4fe7-8bdf-813391137a97"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Type: torch.FloatTensor\n",
            "Shape/size: torch.Size([3, 4])\n",
            "Values: \n",
            "tensor([[ 0.0461,  0.4024, -1.0115,  0.2167],\n",
            "        [-0.6123,  0.5036,  0.2310,  0.6931],\n",
            "        [-0.2669,  2.1785,  0.1021, -0.2590]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Finally, some exercises:\n",
        "\n",
        "#### Simple tensor and autograd example:\n",
        "\n",
        "The following represents a linear equation with scalar variables:\n",
        "\n",
        "$$\n",
        "y = 2x + 3\n",
        "$$"
      ],
      "metadata": {
        "id": "EdUclztScoC8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create tensors.\n",
        "x = torch.tensor(1., requires_grad=True)\n",
        "w = torch.tensor(2., requires_grad=True)\n",
        "b = torch.tensor(3., requires_grad=True)\n",
        "\n",
        "# Build a computational graph.\n",
        "y = w * x + b    # y = 2 * x + 3"
      ],
      "metadata": {
        "id": "D_Et7Nx0c66G"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q:** For $x=1$, how would one compute $\\frac{dy}{dx}$? What about $\\frac{dy}{dw}$?"
      ],
      "metadata": {
        "id": "7IKlbzThd-uD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute gradients.\n",
        "y.backward()\n",
        "# Verify the gradients.\n",
        "assert x.grad == 2, x.grad\n",
        "assert w.grad == 1, w.grad\n",
        "print(x.grad, w.grad)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nWR-7n-Xd_Qo",
        "outputId": "82896114-bd3f-4846-b0a6-c9f15c35f5b8"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(2.) tensor(1.)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "OA_WeLaveK1u"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Building our own `nn.Linear` layer:\n",
        "\n",
        "As you probably know, a linear layer generalizes the computation above to support input vectors and weight matrices\n",
        "\n",
        "$$\n",
        "y = Wx + b\n",
        "$$\n",
        "\n",
        "Let's build our own! Create an `nn.Module` `MyLinear` which on a forward pass takes a `Tensor x` and computes `y` according to learned a learned `weight` and `bias` matrix. Any initialization method is ok."
      ],
      "metadata": {
        "id": "uOYqWo5VhuQo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# need to sub-class nn.Module\n",
        "from torch import nn, Tensor\n",
        "\n",
        "\n",
        "class MyLinear(nn.Module):\n",
        "\n",
        "    # need to implement initialization:\n",
        "    # what parameters do we need to accept? What fields do we use them to define?\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        pass\n",
        "\n",
        "    # calculate our output y = wx + b and returnn\n",
        "    def forward(self, x: Tensor) -> Tensor:\n",
        "        pass\n",
        "\n",
        "# verify:\n",
        "my_linear = MyLinear(4, 5)\n",
        "real_linear = nn.Linear(4, 5)\n",
        "# surgery for equivalent weight and bias\n",
        "my_linear.weight, my_linear.bias = real_linear.weight, real_linear.bias\n",
        "x = torch.randn(4,)\n",
        "assert torch.equal(my_linear(x), real_linear(x))"
      ],
      "metadata": {
        "id": "vARvNNB7iRtM",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        },
        "outputId": "e102de8d-f6d3-4291-db27-04c72f364799"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-bee19381c6ca>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;31m# verify:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0mmy_linear\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMyLinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0mreal_linear\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;31m# surgery for equivalent weight and bias\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: __init__() takes 1 positional argument but 3 were given"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## A longer exercise (borrowed from [Stanford CS 224n](https://colab.research.google.com/drive/13HGy3-uIIy1KD_WFhG4nVrxJC-3nUUkP?usp=sharing) )\n",
        "\n",
        "\n",
        "## Word Window Classification\n",
        "\n",
        "Until this part of the notebook, we have learned the fundamentals of PyTorch and built our own linear layeer. Now we will attempt to solve an example NLP task. Here are the things we will learn:\n",
        "\n",
        "1. Data: Creating a Dataset of Batched Tensors\n",
        "2. Modeling\n",
        "3. Training\n",
        "4. Prediction\n",
        "\n",
        "In this section, our goal will be to train a model that will find the words in a sentence corresponding to a `LOCATION`, which will be always of span `1` (meaning that `San Fransisco` won't be recognized as a `LOCATION`). Our task is called `Word Window Classification` for a reason. Instead of letting our model to only take a look at one word in each forward pass, we would like it to be able to consider the context of the word in question. That is, for each word, we want our model to be aware of the surrounding words. Let's dive in!\n",
        "\n",
        "**Q:** What is our output space?"
      ],
      "metadata": {
        "id": "HTzaoT1rhmNV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Part 1: Loading Data as a [Custom Dataset](https://pytorch.org/tutorials/beginner/data_loading_tutorial.html#dataset-class)\n",
        "\n",
        "Let's say I give two files `train_data.json` and `test_data.json`. Each file contains a `List` of Dictionaries, with a `sentence` and a `label`, where `sentence` is a string and `label` is a list of integers labelling each word in the sentence (determined by whitespace) as being a location or not.\n",
        "\n",
        "Write a Custom dataset class `WWCDataset` for loading and using this data with pytorch, accepting one of the files as input. E.g:\n",
        "\n",
        "`train_dataset: WWCDataset = WWCDataset(\"train_data.json\")` should load the data as expected by PyTorch. Results from `__getitem__` should be preprocessed using the provided function. **Since this dataset is small, you can load it into memory inits entirety as needed**."
      ],
      "metadata": {
        "id": "HotcU0KqvUwG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ================================================================== #\n",
        "#                Input pipeline for custom dataset                 #\n",
        "# ================================================================== #\n",
        "\n",
        "# You should build your custom dataset as below.\n",
        "import torch\n",
        "\n",
        "\n",
        "def preprocess_sentence(sentence):\n",
        "  return sentence.lower().split()\n",
        "\n",
        "\n",
        "class WWCDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self):\n",
        "        # TODO: Load the data from the target file\n",
        "        # TODO: Construct a Vocabulary: a dictionary of all (processed) words to an integer between 0 and |V|\n",
        "        # TODO: Add tokens <unk> and <pad> to this vocabulary. Set <pad> to have its value be zero in the dict!\n",
        "        pass\n",
        "\n",
        "    def __getitem__(self, index: int):\n",
        "        # TODO: Select the item from our set for index\n",
        "        # TODO: preprocess sentence with the function above\n",
        "        # TODO add the window_size of <pad> token to the left and right of each the sentence\n",
        "        # TODO: convert our sentence to a tensor of integers using the vocabulary\n",
        "        # TODO: return a dict {\"sentence\" : Tensor, \"label\": Tensor}\n",
        "        pass\n",
        "\n",
        "    def __len__(self):\n",
        "        # TODO: change 0 to the total size of your dataset.\n",
        "        return 0\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    train_dataset: WWCDataset = WWCDataset(\"data/train_data.json\")\n",
        "    print(train_dataset[11])"
      ],
      "metadata": {
        "id": "gSyGdkmMvb3_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# You can test with the following:\n",
        "train_dataset: WWCDataset = WWCDataset(\"data/train_data.json\")\n",
        "train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
        "                                            batch_size=1,\n",
        "                                            shuffle=True)\n",
        "for item in train_loader:\n",
        "    print(item)"
      ],
      "metadata": {
        "id": "WB-2f7EiwdWj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Aside: \n",
        "\n",
        "Notice while the above works, it only works for a batch size of 1. Trying a larger batch size fails!"
      ],
      "metadata": {
        "id": "gYXk3mDo6MZ9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "    # You can test with the following:\n",
        "    train_dataset: WWCDataset = WWCDataset(\"data/train_data.json\")\n",
        "    train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
        "                                            batch_size=8,\n",
        "                                            shuffle=True)\n",
        "    for item in train_loader:\n",
        "        print(item)\n",
        "except RuntimeError as e:\n",
        "    print(type(e), e)"
      ],
      "metadata": {
        "id": "2OsvmlCA7NhB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**We need to pad our sequences!** To do this, we'll have to define a **custom collate function**. For now though, we'll skip over this and just get a working prototype with batch sizes of 1."
      ],
      "metadata": {
        "id": "0GoeccIz8b1g"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "MKV5E7N48oni"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 2: Training\n",
        "\n",
        "Now we have a dataloader that works for batch size 1, we can try to train a model.\n",
        "\n",
        "Our model will work as follows:\n",
        "- for each word in a sentence, take the two words to the left and"
      ],
      "metadata": {
        "id": "lRze8_wZ-Ufn"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "JdCJiYBC-hXJ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}